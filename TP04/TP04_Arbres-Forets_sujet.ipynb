{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP04. Arbres de décision et Forêts aléatoires\n",
    "\n",
    "Dans ce TP, nous allons traiter les arbres de décision ainsi que les forêts aléatoires.\n",
    "Dans ce TP, nous allons implémenter ID3 pour les caracéristiques nominales et CART (classement) pour les caracéristiques numériques seulement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binômes : \n",
    "- **Binôme 1 :** \n",
    "- **Binôme 2 :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.21.4', '1.4.3', '3.4.3')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type, Union\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "Il existe plusieurs implémentations des arbres de décision :\n",
    "- ID3 (Iterative Dichotomiser 3): dévelopé en 1986 par Ross Quinlan. Il peut être appliqué seulement sur les caractéristiques nominales. Il est utilisé pour le classement.\n",
    "- C4.5: une extension de ID3 par Ross Quinlan. Il peut être appliqué sur tous les types de caractéristiques. Il est utilisé pour le classement.\n",
    "- C5.0: une extension commerciale de C4.5, toujours par Ross Quinlan.\n",
    "- CART (Classification and Regression Trees): comme C4.5 mais utilise d'autres métriques. Aussi, l'algorithme supporte la régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension des algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, nous allons utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. ID3\n",
    "\n",
    "Ici, nous allons implémenter l'algorithme vu dans le cours. \n",
    "Nous allons utiliser le dataset \"jouer (nominales)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le dataset \"jouer\". \n",
    "\n",
    "# temps, temperature, humidite, vent\n",
    "X_jouer = np.array([\n",
    "    ['ensoleile', 'chaude' , 'haute'  , 'non'],\n",
    "    ['ensoleile', 'chaude' , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 'chaude' , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 'fraiche', 'normale', 'non'],\n",
    "    ['pluvieux' , 'fraiche', 'normale', 'oui'],\n",
    "    ['nuageux'  , 'fraiche', 'normale', 'oui'],\n",
    "    ['ensoleile', 'douce'  , 'haute'  , 'non'],\n",
    "    ['ensoleile', 'fraiche', 'normale', 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'normale', 'non'],\n",
    "    ['ensoleile', 'douce'  , 'normale', 'oui'],\n",
    "    ['nuageux'  , 'douce'  , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 'chaude' , 'normale', 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'haute'  , 'oui']\n",
    "])\n",
    "\n",
    "Y_jouer = np.array(['non', 'non', 'oui', 'oui', 'oui', 'non', 'oui', \n",
    "                    'non', 'oui', 'oui', 'oui', 'oui', 'oui', 'non'])\n",
    "\n",
    "len(X_jouer), len(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.1. Probabilité \n",
    "\n",
    "Etant donné une liste des valeurs $S$, la probabilité d'occurence d'une valeur $v$  est le nombre d'occurence de $v$ dans $S$ divisé par le nombre total des éléments de $S$. \n",
    "\n",
    "$$p(v/S) = \\frac{|\\{x / x \\in S \\text{ et } x = v\\}|}{|S|}$$\n",
    "\n",
    "Exemple, prenons la colonne \"jouer\". \n",
    "Le nombre de \"oui\" est 9 et le nombre total est 14. \n",
    "$$p(jouer=oui) = \\frac{9}{14} = 0.6428571428571429$$\n",
    "\n",
    "Voici les paramètres de la fontions :\n",
    "- **S** : un vecteur des valeurs nominales\n",
    "- **v** : une valeur donnée\n",
    "- **résultat** : probabilité d'occurrence de la valeur **v** dans l'ensemble **S**\n",
    "\n",
    "**P.S.** : si la division retourne toujours 0, essayer d'appliquer float(x) sur le numérateur ou le dénominateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6428571428571429,\n",
       " 0.35714285714285715,\n",
       " 0.2857142857142857,\n",
       " 0.35714285714285715)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Probabilité d'occurence d'une valeur dans un ensemble\n",
    "def P(S: np.ndarray, v: str) -> float: \n",
    "    return np.count_nonzero(S==v)/len(S)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.6428571428571429,\n",
    "#  0.35714285714285715,\n",
    "#  0.2857142857142857,\n",
    "#  0.35714285714285715)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "P(Y_jouer      , 'oui'      ), \\\n",
    "P(X_jouer[:, 0], 'ensoleile'), \\\n",
    "P(X_jouer[:, 0], 'nuageux'  ), \\\n",
    "P(X_jouer[:, 0], 'pluvieux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Incertitude d'un ensemble\n",
    "\n",
    "L'entropie de Shannon correspond à la quantité d'information contenue dans une source d'information ; plus la source émet d'informations différentes, plus l'entropie (ou l'incertitude sur ce que la source émet) est grande.\n",
    "Donc, un ensemble avec une entropie de 0 contient les mêmes valeurs.\n",
    "Etant donné : \n",
    "- $S$ une liste des valeurs \n",
    "- $V$ un ensemble de valeurs uniques de $S$ (vocabulaire) \n",
    "\n",
    "L'entropie de $S$ est calculée comme suit : \n",
    "$$H(S) = - \\sum\\limits_{v \\in V} p(v/S) \\log_2 p(v/S)$$\n",
    "\n",
    "Par exemple, la colonne \"jouer\" contient deux valeurs \"oui\" et \"non\". \n",
    "Son entopie est :\n",
    "$$H(jouer) = - \\frac{9}{14} * \\log_2(\\frac{9}{14}) - \\frac{5}{14} * \\log_2(\\frac{5}{14}) = 0.9402859586706309$$ \n",
    "\n",
    "\n",
    "**P.S.** : np.log2 calcule log2 d'une valeur, vecteur ou matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9402859586706311, 1.5774062828523454)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Entropie\n",
    "def H(S: np.ndarray) -> float: \n",
    "    V = np.unique(S)\n",
    "    entropie = 0\n",
    "    # Compléter ici\n",
    "    for i in V:\n",
    "        entropie+=P(S,i)*np.log2(P(S,i))\n",
    "    return -entropie\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.9402859586706311, 1.5774062828523454)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H(Y_jouer), H(X_jouer[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Division d'un ensemble\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, on essaye de diviser la liste des prédictions (classes) selon les valeurs d'un attribut (caractéristique, colonne) à des sous listes. \n",
    "\n",
    "Etant donné : \n",
    "- **Y** : la liste à diviser\n",
    "- **A** : la liste des valeurs d'un attribut (caractéristique, colonne). C'est un vecteur aligné avec Y ; c-à-d, chaque élément de A a un élément de Y respectif.\n",
    "- **v** : la valeur sur laquelle on divie.\n",
    "\n",
    "$$S_{A,v} = \\{y^{(i)} \\in Y / a^{(i)} \\in A \\wedge a^{(i)} = v\\}\\}$$\n",
    "\n",
    "Par exemple, si \n",
    "- $Y$ est la liste des prédictions de \"jouer\"\n",
    "- $A$ est la liste des valeurs de la caractéristique \"temps\"\n",
    "- $v$ est la valeur \"ensoleile\"\n",
    "\n",
    "Le sous ensemble de \"jouer\" où (temps = \"ensoleile\") contient 3 non et 2 oui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non', 'non', 'non', 'oui', 'oui'], dtype='<U3')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diviser_ID3(Y: np.ndarray, A: np.ndarray, v: str) -> np.ndarray:\n",
    "    msk = A == v\n",
    "    return Y[msk]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array(['non', 'non', 'non', 'oui', 'oui'], dtype='<U3')\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "diviser_ID3(Y_jouer, X_jouer[:,0], 'ensoleile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. Gain d'entropie\n",
    "\n",
    "Le gain d'entropie (information gain) est la différence entre l'entropie avant et après la division d'une liste $Y$ selon l'attribut $A$. \n",
    "En d'autres termes, combien d'incertitude dans $Y$ a été réduite après sa division en utilisant l'attribut $A$.\n",
    "\n",
    "Etant donné : \n",
    "- **Y** : une liste à diviser\n",
    "- **A** : une liste des valeurs d'un attribut (caractéristique, colonne) \n",
    "- **V** : l'ensemble des valeurs différentes de l'attribut A (vocabulaire)\n",
    "- **p(v/A)** : la probabité d'occurence de la valeur $v$ dans $A$\n",
    "- $Y_{A, v}$ : sous-ensemble de $Y$ où les valeurs de $V$ égalent à $v$  en utilisant la fonction précédente (diviser_ID3)\n",
    "\n",
    "Le gain d'entrepie est calculé comme suit : \n",
    "\n",
    "$$IG(Y, A) = H(Y) - \\sum_{v \\in V} p(v/A) H(Y_{A, v})$$\n",
    "\n",
    "Les paramètres de la fonction :\n",
    "- **Y** : un vecteur des valeurs à diviser\n",
    "- **A** : un vecteur d'une caratéristique **A** sur laquelle nous voulons diviser **Y**\n",
    "- **Résultat** : un tuple (gain d'entropie, entropie). On rend l'entropie pour ne pas recalculer ultérierement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5935178892225352, 0.9402859586706311)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gain d'entropie\n",
    "def IG(Y: np.ndarray, A: np.ndarray) -> Tuple[float, float]:\n",
    "    V = np.unique(A)\n",
    "    entropie = H(Y)\n",
    "    ig_global = entropie\n",
    "    # Compléter ici\n",
    "    for i in V:\n",
    "        sum_p=P(A, i)* H(diviser_ID3(Y,A,i))\n",
    "    \n",
    "    return entropie-sum_p, entropie\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.24674981977443933, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "IG(Y_jouer, X_jouer[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.5. Choix de l'attribut de division ID3\n",
    "\n",
    "Ici, nous devons trouver l'attribut qui maximise IG.\n",
    "\n",
    "$$jj = \\arg\\max_j IG(Y, X_j)$$\n",
    "\n",
    "Les paramètres de la fonction :\n",
    "- **X[M, N]** : une matrice de M échantillons et de N caractéristiques nominales.\n",
    "- **Y[M]** : un vecteur des classes\n",
    "- **Résultat** : un tuple (numéro d'attribut, IG, entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9402859586706311, 0.9402859586706311)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Choix d'attribut de dévision\n",
    "def choisir_devision_ID3(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]: \n",
    "    jj = -1 # numéro d'attribut qui maximise IG\n",
    "    ig_jj = -1.0 # IG de cet attribut (le max)\n",
    "    h_jj = -1.0  # Entropie \n",
    "    # Compléter ici\n",
    "    for i in range(X.shape[1]):\n",
    "        ig=IG(Y, X[:,i])\n",
    "    jj=np.argmax(ig)\n",
    "    ig_jj=ig[jj]\n",
    "    h_jj=H(Y)   \n",
    "        \n",
    "\n",
    "\n",
    "    return jj, ig_jj,h_jj\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0, 0.24674981977443933, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_devision_ID3(X_jouer, Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.6. Arrêt de division\n",
    "\n",
    "Etant donné les données suivantes :\n",
    "- **Y** : l'ensemble des prédiction au niveau d'un noeud\n",
    "- **h** : le critère d'homoginiété. h = 0 ==> l'ensemble Y est homogène (mêmes valeurs)\n",
    "- **nbr_min** : le nombre minimale des observations dans un noeud. |Y| <= nbr_min ==> le noeud doit être une feuille\n",
    "\n",
    "La fonction d'arrêt doit retourner : \n",
    "- Le nom de la classe : si l'ensemble est homogène ou il contient un nombre minimal des éléments\n",
    "- None : sinon\n",
    "\n",
    "**HINT** : utiliser numpy.unique et numpy.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['oui'], dtype='<U3'), array(['non', 'oui'], dtype='<U3'), None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Arrêt ID3\n",
    "def arreter_ID3(Y: np.ndarray, h: float, nbr_min: int) -> Union[str, None]:\n",
    "    if ((h==0)|(nbr_min>=len(Y))):\n",
    "        return np.unique(Y)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ('oui', 'non', None)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Y_t1 = np.array(['oui', 'oui', 'oui'])\n",
    "Y_t2 = np.array(['oui', 'non', 'non'])\n",
    "\n",
    "arreter_ID3(Y_t1, H(Y_t1), 2), \\\n",
    "arreter_ID3(Y_t2, H(Y_t2), 4), \\\n",
    "arreter_ID3(Y_t2, H(Y_t2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.7. Création de l'arbre\n",
    "\n",
    "**Rien à programmer ici**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1100/726249974.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;31m#---------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0marbre_jouer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentrainer_ID3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_jouer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_jouer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Le Code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1100/726249974.py\u001b[0m in \u001b[0;36mentrainer_ID3\u001b[1;34m(X, Y, nbr_min, profondeur)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Chercher la meilleure caractéristique de X pour diviser Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mjj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mig_jj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_jj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoisir_devision_ID3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Créer un noeud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mnoeud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNoeud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mig_jj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_jj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofondeur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Une classe pour contenir les informations du noeud \n",
    "# et la liste de ces fils\n",
    "class Noeud(object): \n",
    "    \n",
    "    nbr = 0\n",
    "    \n",
    "    def __init__(self, num:int, ig:float, h:float, profondeur:int): \n",
    "        self.num    = num        # le numéro du caractéristique de dévision dans X\n",
    "        self.ig     = ig         # le IG de division\n",
    "        self.h      = h          # l'entropie H\n",
    "        self.pr     = profondeur # la profondeur du noeud\n",
    "        self.fils   = {}         # les fils ; un dictionnaire valeur : noeud\n",
    "        self.cls    = ''         # la classe si ce noeud est final (s'il n'y a pas de fils)\n",
    "        self.indent = '    '     # indentation lorsqu'on génère le code\n",
    "    \n",
    "    # Cette fonction est pour transformer le noeud à une string\n",
    "    #Ici, nous avons redéfini cette fonction afin qu'elle écrive l'arbre \n",
    "    #sous form d'un algorithme ; c'est un parser \n",
    "    def __str__(self):\n",
    "        \n",
    "        indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return indent + 'Y est \"' + self.cls + '\"\\n'\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res = \"\"\n",
    "        for valeur in self.fils:\n",
    "            res += indent + 'Si X[' + str(self.num) + '] est \"' + str(valeur) \n",
    "            res += '\" Alors\\n' + str(self.fils[valeur])\n",
    "        return res\n",
    "    \n",
    "    # predire un échantillon\n",
    "    def predire(self, x: List[str]) -> str: \n",
    "        \n",
    "        # Si le noeud est final, il rend sa classe \n",
    "        if (len(self.fils)==0):\n",
    "            return self.cls\n",
    "        \n",
    "        # Si la valeur de la colonne respective à ce noeud n'appartient pas à l'ensemble des\n",
    "        # valeurs attendues, on rend np.nan\n",
    "        if x[self.num] not in self.fils: \n",
    "            return np.nan\n",
    "        \n",
    "        # Sinon, on rend \n",
    "        return self.fils[x[self.num]].predire(x)\n",
    "    \n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self): \n",
    "        \n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res  = nid + '[label=\"X[' + str(self.num) + ']\\\\n'\n",
    "        res += 'H = ' + str(self.h) + '\\\\n'\n",
    "        res += 'IG = ' + str(self.ig) + '\"];\\n'\n",
    "        for valeur in self.fils:\n",
    "            vid, code = self.fils[valeur].graphviz()\n",
    "            res += code\n",
    "            res += nid + ' -> ' + vid + ' [label=\"' + valeur + '\"];\\n'\n",
    "        return nid, res\n",
    "    \n",
    "\n",
    "# créer l'arbre de décision à partir d'un ensemble X et Y\n",
    "def entrainer_ID3(X:np.ndarray, Y:np.ndarray, nbr_min:int=0, profondeur:int=0): \n",
    "    \n",
    "    # Chercher la meilleure caractéristique de X pour diviser Y\n",
    "    jj, ig_jj, h_jj = choisir_devision_ID3(X, Y)\n",
    "    # Créer un noeud\n",
    "    noeud = Noeud(jj, ig_jj, h_jj, profondeur)\n",
    "    # si arrêter rend une classe, donc c'est une feuille \n",
    "    cls = arreter_ID3(Y, h_jj, nbr_min)\n",
    "    if cls:\n",
    "        noeud.cls = cls # la classe du noeud\n",
    "        return noeud # retourner le noeud \n",
    "    \n",
    "    # Sinon, si le noeud n'est pas une feuille, on crée ces fils\n",
    "    profondeur += 1 # la profondeur de ces fils\n",
    "    # les fils sont créés à partir des valeurs uniques du meilleur caractéristique\n",
    "    for v in np.unique(X[:, jj]):\n",
    "        # Ces trois lignes sont pour récupérer les sous-ensembles X_val, Y_val\n",
    "        # Corresondants à une valeur du meilleur caractéristique\n",
    "        msk = X[:, jj] == v \n",
    "        X_v = X[msk]\n",
    "        Y_v = Y[msk]\n",
    "        # On refait la même opération sur l'ensemble (Y_val) d'une manière récursive\n",
    "        fils = entrainer_ID3(X_v, Y_v, nbr_min=nbr_min, profondeur=profondeur)\n",
    "        # On affecte le noeud créé indexé par la valeur du meilleur caractéristique \n",
    "        # à l'ensemble des fils du noeud courant\n",
    "        noeud.fils[v] = fils\n",
    "    \n",
    "    return noeud\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Le Code\n",
    "# Si X[0] est \"ensoleile\" Alors\n",
    "#     Si X[2] est \"haute\" Alors\n",
    "#         Y est \"non\"\n",
    "#     Si X[2] est \"normale\" Alors\n",
    "#         Y est \"oui\"\n",
    "# Si X[0] est \"nuageux\" Alors\n",
    "#     Y est \"oui\"\n",
    "# Si X[0] est \"pluvieux\" Alors\n",
    "#     Si X[3] est \"non\" Alors\n",
    "#         Y est \"oui\"\n",
    "#     Si X[3] est \"oui\" Alors\n",
    "#         Y est \"non\"\n",
    "#\n",
    "# 'oui'\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "arbre_jouer = entrainer_ID3(X_jouer, Y_jouer)\n",
    "\n",
    "print('Le Code')\n",
    "print(arbre_jouer)\n",
    "\n",
    "# Tester sur un échantillon\n",
    "arbre_jouer.predire(['pluvieux', 'temperature_makanche', 'humidite_makanche', 'non'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.8. Regrouper le tous\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(object): \n",
    "    \n",
    "    def entrainer(self, X, Y, X_noms=[], Y_nom='', nbr_min=0):\n",
    "        self.arbre = entrainer_ID3(X, Y, nbr_min=nbr_min)\n",
    "        code = str(self.arbre)\n",
    "        if len(Y_nom) > 0: \n",
    "            code = code.replace('Y', Y_nom)\n",
    "        for i in range(len(X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', X_noms[i])\n",
    "        self.code = code\n",
    "        self.X_noms = X_noms\n",
    "    \n",
    "    def predire(self, X): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.arbre.predire(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, code = self.arbre.graphviz()\n",
    "        res  = 'digraph Tree {\\n'\n",
    "        res += 'node [shape=box] ;'\n",
    "        for i in range(len(self.X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', self.X_noms[i])\n",
    "        res += code\n",
    "        res += '}'\n",
    "        return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Si temps est \"ensoleile\" Alors\n",
    "#     Si humidite est \"haute\" Alors\n",
    "#         jouer est \"non\"\n",
    "#     Si humidite est \"normale\" Alors\n",
    "#         jouer est \"oui\"\n",
    "# Si temps est \"nuageux\" Alors\n",
    "#     jouer est \"oui\"\n",
    "# Si temps est \"pluvieux\" Alors\n",
    "#     Si vent est \"non\" Alors\n",
    "#         jouer est \"oui\"\n",
    "#     Si vent est \"oui\" Alors\n",
    "#         jouer est \"non\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "id3_classifieur = ID3()\n",
    "id3_classifieur.entrainer(X_jouer, Y_jouer, X_noms=['temps', 'temperature', 'humidite', 'vent'], Y_nom='jouer')\n",
    "print(id3_classifieur.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. CART\n",
    "\n",
    "Ici, nous allons implémenter l'algorithme CART pour la classification avec des caractéristiques numériques. \n",
    "Nous allons utiliser le dataset \"jouer (numériques)\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature, humidite, vent\n",
    "X_njouer = np.array([\n",
    "    [30, 85, 0],\n",
    "    [27, 90, 1],\n",
    "    [28, 78, 0],\n",
    "    [21, 96, 0],\n",
    "    [20, 80, 0],\n",
    "    [18, 70, 1],\n",
    "    [18, 65, 1],\n",
    "    [22, 95, 0],\n",
    "    [21, 70, 0],\n",
    "    [24, 80, 0],\n",
    "    [24, 70, 1],\n",
    "    [22, 90, 1],\n",
    "    [27, 75, 0],\n",
    "    [22, 80, 1]\n",
    "])\n",
    "\n",
    "Y_njouer = Y_jouer \n",
    "\n",
    "len(X_njouer), len(Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.1. Index de diversité de Gini\n",
    "\n",
    "Dans le cas de classement, CART utilise l'indexe de diversité Gini pour mesurer l'erreur de classification.\n",
    "Un index de 0 représente la meilleure division; \n",
    "\n",
    "Etant donné : \n",
    "- $S$ liste des valeurs  \n",
    "- $V$ ensemble des valeurs uniques de $S$  (vocabulaire)\n",
    "\n",
    "L'index de diversité  $Gini(S)$ est calculée comme suit : \n",
    "$$Gini(S) = \\sum\\limits_{v \\in V} p(v/S) (1-p(v/S)) = 1 - \\sum\\limits_{v \\in V} p(v/S)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gini\n",
    "def Gini(S: np.ndarray) -> float:  \n",
    "    V = np.unique(S)\n",
    "    gini = 1\n",
    "    # Compléter ici\n",
    "    return gini \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.4591836734693877\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Gini(Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Division d'un ensemble\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, nous essayons de diviser la liste des prédictions (classes) $Y$ selon une valeur donnée $v$ d'un attribut (caractéristique, colonne) $A$ sur deux listes :\n",
    "- $Y_G$ : une liste contenant les éléments de $Y$ où $A > v$\n",
    "- $Y_D$ : une liste contenant les éléments de $Y$ où $A \\le v$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diviser_CART(Y:np.ndarray, A:np.ndarray, v:float):\n",
    "    msk = A > v\n",
    "    return Y[msk], Y[~msk]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array(['non', 'non', 'oui', 'oui', 'non', 'oui', 'oui', 'oui', 'oui',\n",
    "#         'oui', 'non'], dtype='<U3'),\n",
    "#  array(['oui', 'non', 'oui'], dtype='<U3'))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "diviser_CART(Y_njouer, X_njouer[:,0], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. Diversité Gini de la division\n",
    "\n",
    "Etant donné : \n",
    "- **Y** : une liste de prédictions\n",
    "- **V** : les différentes valeurs de Y (les classes\n",
    "- S_G, G_D : sous ensembles gauche et droit\n",
    "- $|S| = |S_G| + |S_D|$\n",
    "\n",
    "La diversité Gini de la division : \n",
    "\n",
    "$$Gini_{div}(S_G, S_D) = \\frac{|S_G|}{|S|} Gini(S_G) + \\frac{|S_D|}{|S|} Gini(S_D)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Diversité gini de la division\n",
    "def Gini_div(S_G: np.ndarray, S_D: np.ndarray) -> float:  \n",
    "    S_len = float(len(S_G) + len(S_D)) \n",
    "    # Compléter ici\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 0.4588744588744589\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "S_G, S_D = diviser_CART(Y_njouer, X_njouer[:, 0], 20)\n",
    "Gini_div(S_G, S_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.4. Choix de l'attribut et la valeur de division CART\n",
    "\n",
    "L'algorithme\n",
    "- Pour chaque ccaractéristique $X_j$ \n",
    "   - Pour chaque valeur $v$ appartennant aux valeurs uniques de $X_j$\n",
    "       1. Diviser $Y$ en se basant sur la valeur $v$ et celles de $X_j$\n",
    "       1. Calculer Gini de cette division \n",
    "       1. Garder l'indice **jj** de la caractéristique qui minimise Gini\n",
    "       1. Garder Gini minimale **gini_jj**\n",
    "       1. Garder la valeur de dévision **v_jj**\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choix de l'attribut et la valeur de division CART\n",
    "def choisir_division_CART(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]: \n",
    "    jj      = -1\n",
    "    gini_jj =  1.0\n",
    "    v_jj    = -1.0\n",
    "    # Compléter ici\n",
    "    return jj, gini_jj, v_jj\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1, 0.3936507936507937, 80)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_division_CART(X_njouer, Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.5. Arrêt de division CART\n",
    "\n",
    "**Rien à programmer ici** \n",
    "\n",
    "On va utiliser la même fonction que celle de ID3. Mais, pour être consistant, on va seulement renomer la fonction et passer Gini à la place de l'entropie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arreter_CART = arreter_ID3\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ('oui', 'non', None)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Y_t1 = np.array(['oui', 'oui', 'oui'])\n",
    "Y_t2 = np.array(['oui', 'non', 'non'])\n",
    "arreter_CART(Y_t1, H(Y_t1), 2), \\\n",
    "arreter_CART(Y_t2, H(Y_t2), 4), \\\n",
    "arreter_CART(Y_t2, H(Y_t2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.6. Création de l'arbre\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme Noeud, mais il faut changer un peu, puisqu'on teste sur des \n",
    "# valeurs numériques aussi\n",
    "# Le code sera plus utilisable si on crée une classe commune et on hérite\n",
    "# Mais, je n'ai pas le temps pour tout ça (DEAL WITH IT)\n",
    "class NoeudBin(object): \n",
    "    \n",
    "    nbr = 0\n",
    "    \n",
    "    def __init__(self, num:int, val:int, gini:float, profondeur:int): \n",
    "        self.num    = num        # le numéro du caractéristique de dévision dans X\n",
    "        self.val    = val        # la valeur du noeud\n",
    "        self.gini   = gini       # le Gini de division\n",
    "        self.pr     = profondeur # la profondeur du noeud\n",
    "        self.fils   = []         # les fils ; un tableau de deux noeuds: S_G, S_D\n",
    "        self.cls    = ''         # la classe si ce noeud est final (s'il n'y a pas de fils)\n",
    "        self.indent = '    '     # indentation lorsqu'on génère le code\n",
    "    \n",
    "    # Cette fonction est pour transformer le noeud à une string\n",
    "    #Ici, nous avons redéfini cette fonction afin qu'elle écrive l'arbre \n",
    "    #sous form d'un algorithme ; c'est un parser \n",
    "    def __str__(self):\n",
    "        \n",
    "        indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return indent + 'Y est \"' + self.cls + '\"\\n'\n",
    "         \n",
    "        prefix = ' > '\n",
    "        suffix = ''\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS SINON\n",
    "        res  = ''\n",
    "        res += indent + 'Si X[' + str(self.num) + '] ' + prefix + str(self.val) + suffix \n",
    "        res += ' Alors\\n' + str(self.fils[0])\n",
    "        res += indent + 'Sinon\\n' + str(self.fils[1])\n",
    "        return res\n",
    "    \n",
    "    # predire un échantillon\n",
    "    def predire(self, x: List[float]): \n",
    "        \n",
    "        # Si le noeud est final, il rend sa classe \n",
    "        if (len(self.fils)==0):\n",
    "            return self.cls\n",
    "        \n",
    "        # sinon\n",
    "        if x[self.num] > self.val:\n",
    "            return self.fils[0].predire(x)\n",
    "        return self.fils[1].predire(x)\n",
    "\n",
    "    \n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self): \n",
    "        \n",
    "        nid = 'N' + str(NoeudBin.nbr)\n",
    "        NoeudBin.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        prefix = '] > '\n",
    "        res = nid + '[label=\"X[' + str(self.num) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'Gini = ' + str(self.gini) + '\"];\\n'\n",
    "        vid_G, code_G = self.fils[0].graphviz()\n",
    "        vid_D, code_D = self.fils[1].graphviz()\n",
    "        \n",
    "        res += code_G + code_D\n",
    "        res += nid + ' -> ' + vid_G + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_D + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res\n",
    "\n",
    "# créer l'arbre de décision à partir d'un ensemble X et Y\n",
    "def entrainer_CART(X:np.ndarray, Y:np.ndarray, profondeur:int=0, nbr_min:int=0): \n",
    "    \n",
    "    # Chercher le meilleur caractéristique de X pour diviser Y\n",
    "    jj, gini_jj, v_jj = choisir_division_CART(X, Y)\n",
    "    # Créer un noeud\n",
    "    noeud = NoeudBin(jj, v_jj, gini_jj, profondeur)\n",
    "    # Si l'entropie est 0 donc le noeud est terminal, élagage\n",
    "    \n",
    "    cls = arreter_CART(Y, gini_jj, nbr_min)\n",
    "    if cls:\n",
    "        noeud.cls = cls # la classe du noeud\n",
    "        return noeud # retourner le noeud \n",
    "     \n",
    "    \n",
    "    # Sinon, si le noeud n'est pas terminal, on crée ces fils\n",
    "    profondeur += 1 # la profondeur de ces fils\n",
    "    # création des deux fils\n",
    "    \n",
    "    msk = X[:, jj] > v_jj\n",
    "    \n",
    "    X_G = X[msk]\n",
    "    Y_G = Y[msk]\n",
    "    fils_G = entrainer_CART(X_G, Y_G, profondeur, nbr_min)\n",
    "    X_D = X[~msk]\n",
    "    Y_D = Y[~msk]\n",
    "    fils_D = entrainer_CART(X_D, Y_D, profondeur, nbr_min)\n",
    "    noeud.fils.append(fils_G)\n",
    "    noeud.fils.append(fils_D)\n",
    "    \n",
    "    return noeud\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Le Code\n",
    "# Si X[1]  > 80 Alors\n",
    "#     Si X[0]  > 22 Alors\n",
    "#         Y est \"non\"\n",
    "#     Sinon\n",
    "#         Si X[0]  > 21 Alors\n",
    "#             Y est \"non\"\n",
    "#         Sinon\n",
    "#             Y est \"oui\"\n",
    "# Sinon\n",
    "#     Si X[2]  > 0 Alors\n",
    "#         Si X[1]  > 70 Alors\n",
    "#             Y est \"non\"\n",
    "#         Sinon\n",
    "#             Si X[0]  > 18 Alors\n",
    "#                 Y est \"oui\"\n",
    "#             Sinon\n",
    "#                 Y est \"non\"\n",
    "#     Sinon\n",
    "#         Y est \"oui\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "arbre_njouer = entrainer_CART(X_njouer, Y_njouer)\n",
    "\n",
    "print('Le Code')\n",
    "print(arbre_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.7. Regrouper le tous\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART(object): \n",
    "    \n",
    "    def entrainer(self, X:np.ndarray, Y:np.ndarray, X_noms:List[str]=[], Y_nom:str='', nbr_min:int=0):\n",
    "        self.arbre = entrainer_CART(X, Y, 0, nbr_min)\n",
    "        code = str(self.arbre)\n",
    "        if len(Y_nom) > 0: \n",
    "            code = code.replace('Y', Y_nom)\n",
    "        for i in range(len(X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', X_noms[i])\n",
    "        self.code   = code\n",
    "        self.X_noms = X_noms\n",
    "    \n",
    "    def predire(self, X:np.ndarray): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.arbre.predire(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, code = self.arbre.graphviz()\n",
    "        res  = 'digraph Tree {\\n'\n",
    "        res += 'node [shape=box] ;'\n",
    "        for i in range(len(self.X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', self.X_noms[i])\n",
    "        res += code\n",
    "        res += '}'\n",
    "        return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Si humidite  > 80 Alors\n",
    "#     Si temperature  > 22 Alors\n",
    "#         jouer est \"non\"\n",
    "#     Sinon\n",
    "#         Si temperature  > 21 Alors\n",
    "#             jouer est \"non\"\n",
    "#         Sinon\n",
    "#             jouer est \"oui\"\n",
    "# Sinon\n",
    "#     Si vent  > 0 Alors\n",
    "#         Si humidite  > 70 Alors\n",
    "#             jouer est \"non\"\n",
    "#         Sinon\n",
    "#             Si temperature  > 18 Alors\n",
    "#                 jouer est \"oui\"\n",
    "#             Sinon\n",
    "#                 jouer est \"non\"\n",
    "#     Sinon\n",
    "#         jouer est \"oui\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "cart_classifieur = CART()\n",
    "cart_classifieur.entrainer(X_njouer, Y_njouer, X_noms=['temperature', 'humidite', 'vent'], Y_nom='jouer')\n",
    "print(cart_classifieur.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Nous allons utiliser le dataset [Cars Data](https://www.kaggle.com/abineshkumark/carsdata) pour classer les voitures en trois classes : US., Euroupe. ou Japan. \n",
    "Dans cette section, nous allons discuter les paramètres des arbres de décision et les forêts aléatoires implémentés dans des outils connus (dans ce cas, scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du dataset\n",
    "cars_data = pd.read_csv('data/cars.csv', skipinitialspace=True)\n",
    "# On a remarqué que le type de cette caractéristique n'est pas bien détecté\n",
    "# cars_data['cubicinches'] = pd.to_numeric(cars_data['cubicinches'])\n",
    "# supprimer les valeurs absentes \n",
    "cars_data.dropna(inplace=True)\n",
    "# Yay! We did it! Voici les premières lignes du dataset\n",
    "cars_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_cars = cars_data.values[:, :-1]\n",
    "Y_cars = cars_data.values[:,  -1]\n",
    "\n",
    "X_cars_train, X_cars_test, Y_cars_train, Y_cars_test = train_test_split(X_cars, \n",
    "                                                                        Y_cars, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=0) \n",
    "\n",
    "X_cars_train.shape, X_cars_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Arbres de décision\n",
    "\n",
    "Dans l'implémentation Scikit-learn des arbres de décision (**DecisionTreeClassifier**), les caractéristiques sont permutées d'une façon aléatoire à chaque division. \n",
    "Ceci rendre l'arbre non déterministe. \n",
    "Pour arrêter ça, la proporiété **random_state=0** est utilisée.\n",
    "\n",
    "#### II.1.1. Critère de choix des caractéristiques\n",
    "\n",
    "Nous avons entraîné deux arbres de décision CART avec les critères de division : \n",
    "1. Entropy\n",
    "1. Gini\n",
    "\n",
    "Dans scikit-learn, il n'existe que CART (les arbres sont binaires). \n",
    "Mais le critère de division peut être choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.tree    import DecisionTreeClassifier\n",
    "from   sklearn.metrics import f1_score\n",
    "from   sklearn         import tree\n",
    "import timeit\n",
    "\n",
    "Xchoix_train = X_cars_train\n",
    "Ychoix_train = Y_cars_train\n",
    "Xchoix_test  = X_cars_test\n",
    "Ychoix_test  = Y_cars_test\n",
    "fnames       = cars_data.columns\n",
    "\n",
    "#Xchoix_train = X_njouer[4:, :]\n",
    "#Ychoix_train = Y_njouer[4:   ]\n",
    "#Xchoix_test  = X_njouer[:4, :]\n",
    "#Ychoix_test  = Y_njouer[:4   ]\n",
    "#fnames       = ['temperature', 'humidite', 'vent']\n",
    "\n",
    "\n",
    "gini_stats          = []\n",
    "entropy_stats       = []\n",
    "gini_classifieur    = DecisionTreeClassifier(criterion='gini'   , random_state=0)\n",
    "entropy_classifieur = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "# ============ GINI ====================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "gini_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "gini_stats.append(f1_score(Ychoix_train, gini_classifieur.predict(Xchoix_train), average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = gini_classifieur.predict(Xchoix_test)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "gini_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "# =========== Entropy ==================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "entropy_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "entropy_stats.append(f1_score(Ychoix_train, entropy_classifieur.predict(Xchoix_train), average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = entropy_classifieur.predict(Xchoix_test)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "entropy_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figwidth (20)\n",
    "fig.set_figheight(12)\n",
    "tree.plot_tree(entropy_classifieur, ax=ax1, feature_names=fnames, filled=True)\n",
    "tree.plot_tree(gini_classifieur   , ax=ax2, feature_names=fnames, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Criteres' : ['Temps Entrainement', 'F1 Entrainement', 'Temps Test', 'F1 Test'],\n",
    "    'Entropie' : entropy_stats,\n",
    "    'Gini'     : gini_stats\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la structure des deux arbres : ENTOPY (à gauche) et GINI (à droit) ? La structure veut dire : la profendeur, le nombre des feuilles, la position des feuilles (près de la racine ou non), etc.\n",
    "- Justifier le temps d'entraînement et de test en se basant sur les deux algorithmes et les deux structures discutées.\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.2. Profondeur maximale de l'arbre\n",
    "\n",
    "Pour chaque profondeur, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROF     = 40\n",
    "MAX_PROF_lst = range(1, MAX_PROF+1, 1)\n",
    "\n",
    "f1_train_pm  = []\n",
    "f1_test_pm   = []\n",
    "for max_prof in MAX_PROF_lst:\n",
    "    classifieur = DecisionTreeClassifier(random_state=0, max_depth=max_prof)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_pm.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_pm.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm, color='blue', label='Convergence (entrainement)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm , color='red' , label='Generalisation (Test)'     )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Profondeur max de l\\'arbre')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que plus de profondeur veut dire le modèle va généraliser mieux ?\n",
    "- Justifier\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.3. Observations minimales dans les feuilles\n",
    "\n",
    "Pour chaque nombre des observations minimales dans les feuilles, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FEUILLE     = 40\n",
    "MIN_FEUILLE_lst = range(1, MIN_FEUILLE+1, 1)\n",
    "\n",
    "f1_train_mf     = []\n",
    "f1_test_mf      = []\n",
    "for min_feuille in MIN_FEUILLE_lst:\n",
    "    classifieur = DecisionTreeClassifier(random_state=0, min_samples_leaf=min_feuille)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_mf.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_mf.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf, color='blue', label='Convergence (entrainement)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf , color='red' , label='Generalisation (Test)'     )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Observations minimales dans les feuilles')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Justifier pourquoi la performance se diminue (en indiquant si ce critère d'arrêt garantie un apprentissage normale ou peut causer un sur-apprentissage ou sous-apprentissage)\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Forêts aléatoires\n",
    "\n",
    "#### II.2.1. Nombre des arbres\n",
    "\n",
    "Pour chaque nombre des arbres dans le forêt, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "MAX_ARBRE     = 100\n",
    "MAX_ARBRE_lst = range(1, MAX_ARBRE+1, 1)\n",
    "\n",
    "f1_train_nbarbres = []\n",
    "f1_test_nbarbres  = []\n",
    "for max_arbres in MAX_ARBRE_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=max_arbres)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_nbarbres.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_nbarbres.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_ARBRE_lst, f1_train_nbarbres, color='blue', label='Convergence  (entrainement)')\n",
    "plt.plot(MAX_ARBRE_lst, f1_test_nbarbres , color='red' , label='Generalisation (Test)'      )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Nombre des arbres')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ? (convergence et généralisation)\n",
    "- Est-ce que plus d'arbre PEUT améliorer la performance ? Expliquer.\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2. Profondeur maximale des arbres\n",
    "\n",
    "Pour chaque profondeur, nous entraînons un forêt et nous mesurons sa convergence et sa performance en terme du score F1. \n",
    "Nous comparons les résultats avec les arbres équivalents (avec la même profondeur).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_pm_foret = []\n",
    "f1_test_pm_foret  = []\n",
    "for max_prof in MAX_PROF_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, max_depth=max_prof)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_pm_foret.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_pm_foret.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm_foret, color='blue'  , label='Convergence    (Foret)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm_foret , color='red'   , label='Generalisation (Foret)')\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm      , color='green' , label='Convergence    (Arbre)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm       , color='orange', label='Generalisation (Arbre)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Profondeur max de l\\'arbre')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Comparer la convergence des forêts et des arbres en terme de la profondeur maximale\n",
    "- Comparer la généralisation des forêts et des arbres en terme de la profondeur maximale\n",
    "- Justifier ces résultats (en indiquant pourquoi nous avons des oscillations dans F1 test des forêts)\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3. Observations minimales dans les feuilles\n",
    "\n",
    "Pour chaque nombre minimale des observations dans les feuilles, nous entraînons un forêt et nous mesurons sa convergence et sa performance en terme du score F1. \n",
    "Nous comparons les résultats avec les arbres équivalents (avec le même nombre des observations minimales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_mf_foret = []\n",
    "f1_test_mf_foret  = []\n",
    "for min_feuille in MIN_FEUILLE_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, min_samples_leaf=min_feuille)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_mf_foret.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_mf_foret.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf_foret, color='blue'  , label='Convergence    (Foret)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf_foret , color='red'   , label='Generalisation (Foret)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf      , color='green' , label='Convergence    (Arbre)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf       , color='orange', label='Generalisation (Arbre)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Observations minimales dans les feuilles')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Comparer la performance (convergence et généralisation) des arbres et des forêts en terme du noombre des observations dans les feuilles\n",
    "- Justifier \n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.4. Taille d'un Bootstrap\n",
    "\n",
    "Ici, on définit un pourcentage de la taille des Bootstrap par rapport la taille initiale du dataset. Pour chaque pourcentage, on entraîne un forêt et on test sa performance (convergence et généralisation) en utilisant la mesure F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POUR_OBS_lst = np.arange(0.1, 1, 0.01)\n",
    "\n",
    "f1_train_ech = []\n",
    "f1_test_ech  = []\n",
    "for pour_obs in POUR_OBS_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, max_samples=pour_obs)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_ech.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_ech.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(POUR_OBS_lst, f1_train_ech, color='blue', label='Convergence    (Foret)')\n",
    "plt.plot(POUR_OBS_lst, f1_test_ech , color='red' , label='Generalisation (Foret)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Pourcentage du bootstap par rapport le dataset originale')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Quelle est la raison pour laquelle la performance n'augmente pas d'une manière lisse ?\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "67e0cbc25fa4f5baaacba1240f401bc655b640f8e15cfc935dfee2e63491bdf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
